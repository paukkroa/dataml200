{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "37eb1f5d-41ec-4060-9f20-c28c37ec2b05",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "37eb1f5d-41ec-4060-9f20-c28c37ec2b05",
        "nbgrader": {
          "checksum": "836395fd7ebd63e9652dc2c2beac3c8c",
          "grade": false,
          "grade_id": "cell-341078e8dde7e0e6",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "# Exercise 4: Text Generation using LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e84bca02-56b3-446d-a2b3-a35bcdfc0d34",
      "metadata": {
        "id": "e84bca02-56b3-446d-a2b3-a35bcdfc0d34"
      },
      "outputs": [],
      "source": [
        "skip_training = False   # You can set it to True if you want to run inference on your trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e1996f4f-0290-4073-8f7a-47a30ea8d7c5",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "e1996f4f-0290-4073-8f7a-47a30ea8d7c5",
        "nbgrader": {
          "checksum": "fd6566ff374217876145e2ff34a74cae",
          "grade": false,
          "grade_id": "cell-8d754ffa5b6f36a3",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "af7e51e7-1322-4e83-b9e8-4734601a2c32",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "af7e51e7-1322-4e83-b9e8-4734601a2c32",
        "outputId": "8a8cffef-25b9-4e8a-ceee-d7e6097c07fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66259c62-2979-4b9f-ad1f-bbc76e5bbd29",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "66259c62-2979-4b9f-ad1f-bbc76e5bbd29",
        "nbgrader": {
          "checksum": "e3f0f3acc67cf1db9005351e946bee4c",
          "grade": false,
          "grade_id": "cell-918dec79c2334ca9",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "### 1. Load and Preprocess the Text Dataset\n",
        "\n",
        "We will be using *Alice's Adventures in Wonderland* by Lewis Carroll as our dataset. You can download it from [Project Gutenberg](https://www.gutenberg.org/):\n",
        "\n",
        "[Alice's Adventures in Wonderland by Lewis Carroll (Project Gutenberg Page)](https://www.gutenberg.org/ebooks/11) \\\n",
        "[Direct Text File Download](https://www.gutenberg.org/files/11/11-0.txt)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "39b5e555-045e-4f8f-8150-0f0b69cce823",
      "metadata": {
        "id": "39b5e555-045e-4f8f-8150-0f0b69cce823"
      },
      "outputs": [],
      "source": [
        "txt_path = '/content/alice.txt' # replace 'alice.txt' with your txt path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f1536e4c-1437-432f-b332-ee9e366f34b7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1536e4c-1437-432f-b332-ee9e366f34b7",
        "outputId": "09aa674a-6b93-4bda-a9c7-d0f405d28a44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===First 1500 characters before any processing:\n",
            "\n",
            "\n",
            "*** START OF THE PROJECT GUTENBERG EBOOK 11 ***\n",
            "[Illustration]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Alice’s Adventures in Wonderland\n",
            "\n",
            "by Lewis Carroll\n",
            "\n",
            "THE MILLENNIUM FULCRUM EDITION 3.0\n",
            "\n",
            "Contents\n",
            "\n",
            " CHAPTER I.     Down the Rabbit-Hole\n",
            " CHAPTER II.    The Pool of Tears\n",
            " CHAPTER III.   A Caucus-Race and a Long Tale\n",
            " CHAPTER IV.    The Rabbit Sends in a Little Bill\n",
            " CHAPTER V.     Advice from a Caterpillar\n",
            " CHAPTER VI.    Pig and Pepper\n",
            " CHAPTER VII.   A Mad Tea-Party\n",
            " CHAPTER VIII.  The Queen’s Croquet-Ground\n",
            " CHAPTER IX.    The Mock Turtle’s Story\n",
            " CHAPTER X.     The Lobster Quadrille\n",
            " CHAPTER XI.    Who Stole the Tarts?\n",
            " CHAPTER XII.   Alice’s Evidence\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "CHAPTER I.\n",
            "Down the Rabbit-Hole\n",
            "\n",
            "\n",
            "Alice was beginning to get very tired of sitting by her sister on the\n",
            "bank, and of having nothing to do: once or twice she had peeped into\n",
            "the book her sister was reading, but it had no pictures or\n",
            "conversations in it, “and what is the use of a book,” thought Alice\n",
            "“without pictures or conversations?”\n",
            "\n",
            "So she was considering in her own mind (as well as she could, for the\n",
            "hot day made her feel very sleepy and stupid), whether the pleasure of\n",
            "making a daisy-chain would be worth the trouble of getting up and\n",
            "picking the daisies, when suddenly a White Rabbit with pink eyes ran\n",
            "close by her.\n",
            "\n",
            "There was nothing so _very_ remarkable in that; nor did Alice think it\n",
            "so _very_ much out of the way to hear the Rabbit say to itself, “Oh\n",
            "dear! Oh dear! I shall be late!” (when she thought it over afterwards,\n",
            "it occurred to her that she o\n",
            "\n",
            "\n",
            "\n",
            "===Ending characters before any processing:\n",
            "\n",
            "g.\n",
            "\n",
            "On this the White Rabbit blew three blasts on the trumpet, and then\n",
            "unrolled the parchment scroll, and read as follows:—\n",
            "\n",
            "“The Queen of Hearts, she made some tarts,\n",
            "    All on a summer day:\n",
            "The Knave of Hearts, he stole those tarts,\n",
            "    And took them quite away!”\n",
            "\n",
            "\n",
            "“Consider your verdict,” the King said to the jury.\n",
            "\n",
            "“Not yet, not yet!” the Rabbit hastily interrupted. “There’s a great\n",
            "deal to come before that!”\n",
            "\n",
            "“Call the first witness,” said the King; and the White Rabbit blew\n",
            "three blasts on the trumpet, and called out, “First witness!”\n",
            "\n",
            "The first witness was the Hatter. He came in with a teacup in one hand\n",
            "and a piece of bread-and-butter in the other. “I beg pardon, your\n",
            "Majesty,” he began, “for bringing these in: but I hadn’t quite finished\n",
            "my tea when I was sent for.”\n",
            "\n",
            "“You ought to have finished,” said the King. “When did you begin?”\n",
            "\n",
            "The Hatter looked at the March Hare, who had followed him into the\n",
            "court, arm-in-arm with the Dormouse. “Fourteenth of March, I _think_ it\n",
            "was,” he said.\n",
            "\n",
            "“Fifteenth,” said the March Hare.\n",
            "\n",
            "“Sixteenth,” added the Dormouse.\n",
            "\n",
            "“Write that down,” the King said to the jury, and the jury eagerly\n",
            "wrote down all three dates on their slates, and then added them up, and\n",
            "reduced the answer to shillings and pence.\n",
            "\n",
            "“Take off your hat,” the King said to the Hatter.\n",
            "\n",
            "“It isn’t mine,” said the Hatter.\n",
            "\n",
            "“_Stolen!_” the King exclaimed, turning to the jury, who instantly made\n",
            "a memorandum of the fact.\n",
            "\n",
            "“I keep them to sell,” the Hatter added as an explanation; “I’ve none\n",
            "of my own. I’m a hatter.”\n",
            "\n",
            "Here the Queen put on her spectacles, and began staring at the Hatter,\n",
            "who turned pale and fidgeted.\n",
            "\n",
            "“Give your evidence,” said the King; “and don’t be nervous, or I’ll\n",
            "have you executed on the spot.”\n",
            "\n",
            "This did not seem to encourage the witness at all: he kept shifting\n",
            "from one foot to the other, looking uneasily at the Queen, and in his\n",
            "confusion he bit a large piece out of his teacup instead of the\n",
            "bread-and-butter.\n",
            "\n",
            "Just at this moment Alice felt\n"
          ]
        }
      ],
      "source": [
        "with open(txt_path, 'r') as file:\n",
        "    raw_text = file.read()\n",
        "\n",
        "print('===First 1500 characters before any processing:\\n\\n')\n",
        "print(raw_text[:1500])\n",
        "\n",
        "print('\\n\\n\\n===Ending characters before any processing:\\n')\n",
        "print(raw_text[-19000:-17000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "6fce77de-0ccd-40b1-ac63-3ebff4c9f8ab",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fce77de-0ccd-40b1-ac63-3ebff4c9f8ab",
        "outputId": "ad816a3b-7d6a-4db8-a2c3-fdf1b946a8b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===Text after removing metadata:\n",
            "\n",
            "CHAPTER I.\n",
            "Down the Rabbit-Hole\n",
            "\n",
            "\n",
            "Alice was beginning to get very tired of sitting by her sister on the\n",
            "bank, and of having nothing to do: once or twice she had peeped into\n",
            "the book her sister was reading, but it had no pictures or\n",
            "conversations in it, “and what is the use of a book,” thought Alice\n",
            "“without pictures or conversations?”\n",
            "\n",
            "So she was considering in her own mind (as well as she could, for the\n",
            "hot day made her feel very sleepy and stupid), whether the pleasure of\n",
            "making a daisy-chain would be worth the trouble of getting up and\n",
            "picking the daisies, when suddenly a White Rabbit with pink eyes ran\n",
            "close by her.\n",
            "\n",
            "There was nothing so _very_ remarkable in that; nor did Alice think it\n",
            "so _very_ much out of the way to hear the Rabbit say to itself, “Oh\n",
            "dear! Oh dear! I shall be late!” (when she thought it over afterwards,\n",
            "it occurred to her that she ought to have wondered at this, but at the\n",
            "time it all seemed quite natural); but when the Rabbit actually _took a\n",
            "watch out of its waistcoat-pocket_, and looked at it, and then hurried\n",
            "on, Alice started to her feet, for it flashed across her mind that she\n",
            "had never before seen a rabbit with either a waistcoat-pocket, or a\n",
            "watch to take out of it, and burning with curiosity, she ran across the\n",
            "field after it, and fortunately was just in time to see it pop down a\n",
            "large rabbit-hole under the hedge.\n",
            "\n",
            "In another moment down went Alice after it, never once considering how\n",
            "in the world she was to get out again.\n",
            "\n",
            "The rabbit-hole wen\n"
          ]
        }
      ],
      "source": [
        "# Removing metadata from the text\n",
        "start_index = raw_text.find('CHAPTER I.\\nDown the Rabbit-Hole')\n",
        "\n",
        "end_index = raw_text.find('*** END OF THE PROJECT GUTENBERG') # closing markers of Project Gutenberg\n",
        "\n",
        "trimmed_text = raw_text[start_index:end_index]\n",
        "\n",
        "print('===Text after removing metadata:\\n')\n",
        "print(trimmed_text[:1500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f910909e-acba-45f9-ac49-41c0d3432129",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "id": "f910909e-acba-45f9-ac49-41c0d3432129",
        "nbgrader": {
          "checksum": "1f65b86777609239806c103b9d5d4933",
          "grade": false,
          "grade_id": "cell-6e2568640b9031df",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "outputId": "e744e209-f6ad-45d6-b76f-001615959c19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text after cleaning and converting to lowercase:\n",
            "\n",
            "chapter i down the rabbit hole alice was beginning to get very tired of sitting by her sister on the bank and of having nothing to do once or twice she had peeped into the book her sister was reading but it had no pictures or conversations in it and what is the use of a book thought alice without pictures or conversations so she was considering in her own mind as well as she could for the hot day made her feel very sleepy and stupid whether the pleasure of making a daisy chain would be worth the trouble of getting up and picking the daisies when suddenly a white rabbit with pink eyes ran close by her there was nothing so very remarkable in that nor did alice think it so very much out of the way to hear the rabbit say to itself oh dear oh dear i shall be late when she thought it over afterwards it occurred to her that she ought to have wondered at this but at the time it all seemed quite natural but when the rabbit actually took a watch out of its waistcoat pocket and looked at it and t\n"
          ]
        }
      ],
      "source": [
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Preprocesses the input text by i. converting it to lowercase,\n",
        "    ii. removing non-alphanumeric characters (except spaces),\n",
        "    iii. and normalizing spaces.\n",
        "\n",
        "    Args:\n",
        "    text -- The raw input text as a string\n",
        "\n",
        "    Returns:\n",
        "    cleaned_text -- The processed text where all the preprocessing steps are applied\n",
        "    \"\"\"\n",
        "    # 1. Convert text to lowercase\n",
        "    # 2. Remove special characters\n",
        "    # 3. Remove double spaces\n",
        "    cleaned_text = text.lower()\n",
        "    cleaned_text = re.sub(r'[^a-z0-9\\s]', ' ', cleaned_text)\n",
        "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "cleaned_text = preprocess_text(trimmed_text)\n",
        "print('Text after cleaning and converting to lowercase:\\n')\n",
        "print(cleaned_text[:1000])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7b05133-4168-459e-8e36-2ec88d2ad51d",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "e7b05133-4168-459e-8e36-2ec88d2ad51d",
        "nbgrader": {
          "checksum": "6a920af1dea881be0da01e7f1013c338",
          "grade": false,
          "grade_id": "cell-ae2039f6bc91f976",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "### 2. Character-Level Encoding\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "64011399-7d6d-4cb3-89e8-fa65653a5a72",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "id": "64011399-7d6d-4cb3-89e8-fa65653a5a72",
        "nbgrader": {
          "checksum": "02558911a39ac511871aa6e2c60fe160",
          "grade": false,
          "grade_id": "cell-d2e35b87ae8fa70e",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "outputId": "bae6cbe1-79a0-410b-c4b2-09501b8768d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Character to Integer Mapping:\n",
            "' ' : 0\n",
            "'a' : 1\n",
            "'b' : 2\n",
            "'c' : 3\n",
            "'d' : 4\n",
            "'e' : 5\n",
            "'f' : 6\n",
            "'g' : 7\n",
            "'h' : 8\n",
            "'i' : 9\n",
            "'j' : 10\n",
            "'k' : 11\n",
            "'l' : 12\n",
            "'m' : 13\n",
            "'n' : 14\n",
            "'o' : 15\n",
            "'p' : 16\n",
            "'q' : 17\n",
            "'r' : 18\n",
            "'s' : 19\n",
            "'t' : 20\n",
            "'u' : 21\n",
            "'v' : 22\n",
            "'w' : 23\n",
            "'x' : 24\n",
            "'y' : 25\n",
            "'z' : 26\n"
          ]
        }
      ],
      "source": [
        "def create_char_mappings(cleaned_text):\n",
        "    \"\"\"\n",
        "    Creates character-to-integer and integer-to-character mappings from the cleaned text.\n",
        "\n",
        "    Args:\n",
        "    cleaned_text -- The cleaned input text as a string\n",
        "\n",
        "    Returns:\n",
        "    char_to_int -- A dictionary mapping each unique character to an integer\n",
        "    int_to_char -- A dictionary mapping each integer back to its corresponding character\n",
        "    \"\"\"\n",
        "    unique_chars = sorted(set(cleaned_text))\n",
        "    char_to_int = {char: idx for idx, char in enumerate(unique_chars)}\n",
        "    int_to_char = {idx: char for char, idx in char_to_int.items()}\n",
        "\n",
        "    return char_to_int, int_to_char\n",
        "\n",
        "char_to_int, int_to_char = create_char_mappings(cleaned_text)\n",
        "print('Character to Integer Mapping:')\n",
        "for char, idx in list(char_to_int.items()):\n",
        "    print(f\"'{char}' : {idx}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c33f28d5-de30-4ffc-99cd-ee43a15926f8",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "c33f28d5-de30-4ffc-99cd-ee43a15926f8",
        "nbgrader": {
          "checksum": "5bd13e000e8dca85b863d1014c17d836",
          "grade": false,
          "grade_id": "cell-d3a464179b6eeffb",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "#### 2.2 Encode the Text into Integers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "8bc5efdc-fb21-42c1-92a0-cf559a4b3eea",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "id": "8bc5efdc-fb21-42c1-92a0-cf559a4b3eea",
        "nbgrader": {
          "checksum": "e96be03cbe08258b1a8bb2a4e3e23b20",
          "grade": false,
          "grade_id": "cell-82f52b40eea42a5c",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "outputId": "8af1ba5d-1abf-45a7-8b21-dca1f7fd801d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 100 encoded characters:\n",
            "[ 3  8  1 16 20  5 18  0  9  0  4 15 23 14  0 20  8  5  0 18  1  2  2  9\n",
            " 20  0  8 15 12  5  0  1 12  9  3  5  0 23  1 19  0  2  5  7  9 14 14  9\n",
            " 14  7  0 20 15  0  7  5 20  0 22  5 18 25  0 20  9 18  5  4  0 15  6  0\n",
            " 19  9 20 20  9 14  7  0  2 25  0  8  5 18  0 19  9 19 20  5 18  0 15 14\n",
            "  0 20  8  5]\n"
          ]
        }
      ],
      "source": [
        "def encode_text(cleaned_text, char_to_int):\n",
        "    \"\"\"\n",
        "    Encodes the cleaned text into an array of integers.\n",
        "\n",
        "    Args:\n",
        "    cleaned_text -- The cleaned input text as a string\n",
        "    char_to_int -- Characters to integer mapping\n",
        "\n",
        "    Returns:\n",
        "    encoded_chars -- Numpy array of integers representing the encoded characters from the text\n",
        "    \"\"\"\n",
        "    encoded_chars = np.array([char_to_int[char] for char in cleaned_text])\n",
        "\n",
        "    return encoded_chars\n",
        "\n",
        "encoded_chars = encode_text(cleaned_text, char_to_int)\n",
        "print('First 100 encoded characters:')\n",
        "print(encoded_chars[:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c011421-7952-4c8d-8217-55c04e7d608f",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "6c011421-7952-4c8d-8217-55c04e7d608f",
        "nbgrader": {
          "checksum": "641539c878b4e707dad47e7bacdd6b7b",
          "grade": false,
          "grade_id": "cell-f6f0731068619a8c",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "### 3. Batch Generation for Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "afe5f57f-0bce-428d-8d46-67d71def6237",
      "metadata": {
        "deletable": false,
        "id": "afe5f57f-0bce-428d-8d46-67d71def6237",
        "nbgrader": {
          "checksum": "0699c96caa40d8bbf23ed3a9e1d9d3b4",
          "grade": false,
          "grade_id": "cell-0c4e1828e13b2561",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def get_batches(arr, batch_size, seq_length, step_size=None):\n",
        "    \"\"\"\n",
        "    Generates batches of input and target sequences from the given array.\n",
        "\n",
        "    Args:\n",
        "        arr (array-like): Encoded text as an array of integers.\n",
        "        batch_size (int): Number of sequences per batch.\n",
        "        seq_length (int): Number of characters in each sequence.\n",
        "        step_size (int, optional): Steps to move the window for the next sequence.\n",
        "\n",
        "    Returns:\n",
        "        tuple (x_batches, y_batches): A tuple of numpy arrays of input and target sequences,\n",
        "                                      each has shape (num_batches, batch_size, seq_length)\n",
        "    \"\"\"\n",
        "    if step_size is None:\n",
        "        step_size = seq_length\n",
        "\n",
        "    # Calculate the number of batches\n",
        "    n_batches = (len(arr) - seq_length) // (batch_size * step_size)\n",
        "\n",
        "    # Trim the array to fit the number of batches\n",
        "    arr = arr[:n_batches * batch_size * step_size + seq_length]\n",
        "\n",
        "    # Initialize the batches\n",
        "    x_batches = []\n",
        "    y_batches = []\n",
        "\n",
        "    # Fill batches\n",
        "    for i in range(0, len(arr) - seq_length, step_size):\n",
        "        x = arr[i:i + seq_length]\n",
        "        y = arr[i + 1:i + seq_length + 1]\n",
        "        x_batches.append(x)\n",
        "        y_batches.append(y)\n",
        "\n",
        "    # Convert to numpy arrays and reshape\n",
        "    x_batches = np.array(x_batches).reshape(-1, batch_size, seq_length)\n",
        "    y_batches = np.array(y_batches).reshape(-1, batch_size, seq_length)\n",
        "\n",
        "    return x_batches, y_batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "c7e33b6e-b7f4-45c7-b3bc-14c80d71c4ef",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7e33b6e-b7f4-45c7-b3bc-14c80d71c4ef",
        "outputId": "9dc23e07-c453-44fb-9cbf-bffcf67a9c6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "Displaying a Single Batch\n",
            "==================================================\n",
            "[made her f]  -->  [ade her fe]\n",
            "[her feel v]  -->  [er feel ve]\n",
            "[eel very s]  -->  [el very sl]\n",
            "[ery sleepy]  -->  [ry sleepy ]\n",
            "[leepy and ]  -->  [eepy and s]\n",
            "[ and stupi]  -->  [and stupid]\n",
            "[stupid whe]  -->  [tupid whet]\n",
            "[d whether ]  -->  [ whether t]\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# Display for y shift and  step_size\n",
        "def display_batch_generation(arr, char_to_int, int_to_char):\n",
        "    batch_size, seq_length, step_size = 8, 10, 5  # Setting step_size for overlap between sequences\n",
        "\n",
        "    x_batches, y_batches = get_batches(arr, batch_size, seq_length, step_size)\n",
        "\n",
        "    # Display batch number 10\n",
        "    x_chars = ''.join([int_to_char[idx] for idx in x_batches[10][0]])\n",
        "    y_chars = ''.join([int_to_char[idx] for idx in y_batches[10][0]])\n",
        "\n",
        "    print('='*50)\n",
        "    print('Displaying a Single Batch')\n",
        "    print('='*50)\n",
        "    for i in range(batch_size):\n",
        "        x_chars = ''.join([int_to_char[idx] for idx in x_batches[10][i]])\n",
        "        y_chars = ''.join([int_to_char[idx] for idx in y_batches[10][i]])\n",
        "\n",
        "        print(f\"[{x_chars}]  -->  [{y_chars}]\")\n",
        "    print('='*50)\n",
        "display_batch_generation(encoded_chars, char_to_int, int_to_char )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5662b3c-84f4-402e-a503-ffb1930fdeaa",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "b5662b3c-84f4-402e-a503-ffb1930fdeaa",
        "nbgrader": {
          "checksum": "b407aa7e91d942d51744bd8f60bb70cd",
          "grade": false,
          "grade_id": "cell-63d39ee7de5e409f",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "### 4. Define the Character-Level LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "96e05ab1-1dc9-42cb-9e2a-aeb226861f5a",
      "metadata": {
        "deletable": false,
        "id": "96e05ab1-1dc9-42cb-9e2a-aeb226861f5a",
        "nbgrader": {
          "checksum": "f38a50dd45898543c6efc37941ce9ce0",
          "grade": false,
          "grade_id": "cell-f08538e1fc0fcaff",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "class CharLSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    Character-Level Multi-Layer LSTM Model\n",
        "\n",
        "    This model processes sequences of characters and predicts the next character in the sequence.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_layers, input_dim, hidden_dim, output_dim, dropout_prob):\n",
        "        \"\"\"\n",
        "        Initializes the CharLSTM model with the specified parameters.\n",
        "\n",
        "        Args:\n",
        "            num_layers (int): Number of LSTM layers\n",
        "            input_dim (int): Dimensionality of the input (e.g. one-hot encoded input size)\n",
        "            hidden_dim (int): Dimensionality of the LSTM hidden layer.\n",
        "            output_dim (int): Dimensionality of the output.\n",
        "            dropout_prob (float): Dropout after each layer.\n",
        "        \"\"\"\n",
        "        super(CharLSTM, self).__init__()\n",
        "\n",
        "        # Save hidden dimension and number of layers for hidden state initialization\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Define a single LSTM module configured as a multi-layer LSTM\n",
        "        # Internal dropout of dropout_prob is applied between layers\n",
        "        # Apply additional dropout after LSTM\n",
        "        # Apply dense layer at the end\n",
        "        self.LSTM = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout_prob)\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        \"\"\"\n",
        "        Performs the forward pass of the CharLSTM model.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, seq_length, input_dim).\n",
        "            hidden (tuple): Tuple of (h0, c0), where each is a tensor of shape (num_layers, batch_size, hidden_dim).\n",
        "\n",
        "        Returns:\n",
        "            out (torch.Tensor): Output tensor of shape (batch_size, seq_length, output_dim).\n",
        "            (h, c) (tuple): Updated hidden states (h, c) for each LSTM layer.\n",
        "                - h (torch.Tensor): Final hidden state\n",
        "                - c (torch.Tensor): Final cell state\n",
        "        \"\"\"\n",
        "\n",
        "        # Pass the input through the LSTM\n",
        "        # The LSTM output 'out' has shape (batch_size, seq_length, hidden_dim)\n",
        "        # 'h' and 'c' represent the final hidden and cell states for each layer\n",
        "        out, (h, c)= self.LSTM(x, hidden)\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc(out)\n",
        "\n",
        "        # Return the final output and the updated hidden states\n",
        "        return out, (h, c)\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        \"\"\"\n",
        "        Initializes the hidden and cell states to zeros for each LSTM layer.\n",
        "\n",
        "        Args:\n",
        "            batch_size (int): The batch size for the current data.\n",
        "\n",
        "        Returns:\n",
        "            (h0, c0) (tuple): Tuple of initial hidden states (h0, c0) for each LSTM layer.\n",
        "            - h0 (torch.Tensor): Initial hidden state\n",
        "            - c0 (torch.Tensor): Initial cell state\n",
        "        \"\"\"\n",
        "        # Set the device to match the model's device to prevent device mismatch errors\n",
        "        device = next(self.parameters()).device\n",
        "\n",
        "        # Initialize hidden state (h0) and cell state (c0) to zeros\n",
        "        # Check the expected shape of hidden states in pytorch nn.LSTM documentation\n",
        "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
        "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
        "\n",
        "        return (h0, c0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf5c3c04-852b-4fea-a96d-4440f1eca9f4",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "bf5c3c04-852b-4fea-a96d-4440f1eca9f4",
        "nbgrader": {
          "checksum": "87907c11a292798d8f0afc5ac2e41fd6",
          "grade": false,
          "grade_id": "cell-f811f0b42ac678b1",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "### 5. Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "85869fef-cd2c-4f5f-a34f-21efd2995817",
      "metadata": {
        "deletable": false,
        "id": "85869fef-cd2c-4f5f-a34f-21efd2995817",
        "nbgrader": {
          "checksum": "c3bbd7de1795376183497192a382d6b9",
          "grade": false,
          "grade_id": "cell-12ea36dbb7f387ab",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def train(model, encoded_chars, vocab_size, num_epochs, batch_size,\n",
        "          seq_length, step_size, learning_rate, save_path=None, verbose=True):\n",
        "    \"\"\"\n",
        "    Train the CharLSTM model on encoded text data.\n",
        "\n",
        "    Arguments:\n",
        "    model -- The LSTM model\n",
        "    encoded_chars -- Encoded data (characters)\n",
        "    vocab_size -- Size of the vocabulary\n",
        "    num_epochs -- Number of training epochs\n",
        "    batch_size -- Batch size for training\n",
        "    seq_length -- Sequence length for each batch\n",
        "    learning_rate -- Learning rate for the optimizer\n",
        "    save_path -- Path to save the trained model (optional)\n",
        "    \"\"\"\n",
        "\n",
        "    model.train()  # Set model to training mode\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # Initialize Adam optimizer\n",
        "    criterion = nn.CrossEntropyLoss()  # Cross entropy loss function\n",
        "\n",
        "    # Prepare batches\n",
        "    x_batches, y_batches = get_batches(encoded_chars, batch_size, seq_length, step_size)\n",
        "    num_batches = len(x_batches)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        # Progress bar for the current epoch\n",
        "        batch_loader = tqdm(zip(x_batches, y_batches), total=num_batches,\n",
        "                            leave=True, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
        "\n",
        "        # Initialize hidden states for both LSTM layers\n",
        "        hidden = model.init_hidden(batch_size)\n",
        "\n",
        "        for x, y in batch_loader:\n",
        "            x = torch.as_tensor(x, dtype=torch.long).to(device)\n",
        "            y = torch.as_tensor(y, dtype=torch.long).to(device) # target\n",
        "\n",
        "            hidden = tuple([each.detach() for each in hidden])\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            x_one_hot = F.one_hot(x, num_classes=vocab_size).float()\n",
        "            output, hidden = model(x_one_hot, hidden)\n",
        "            loss = criterion(output.view(-1, vocab_size), y.view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Print the average loss for the current epoch\n",
        "        if verbose:\n",
        "            print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / num_batches:.4f}')\n",
        "\n",
        "        # Optional model saving\n",
        "        # Let us save it each epoch since training takes a while and you want to stop in the middle\n",
        "        if save_path:\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "            print(f'Your trained model at epoch {epoch} is saved successfully!')\n",
        "\n",
        "    return total_loss / num_batches"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f744f6c3-06a4-47a3-9c46-c2344c4e74ab",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "f744f6c3-06a4-47a3-9c46-c2344c4e74ab",
        "nbgrader": {
          "checksum": "78ba413d7fe1f539b5f90fc147ac112a",
          "grade": false,
          "grade_id": "cell-50b02537e194d303",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "#### Model Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "3b68e133-661b-4382-bd4e-b08d6046763e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b68e133-661b-4382-bd4e-b08d6046763e",
        "outputId": "3840bcdf-e9b2-4fa4-a601-aa9d349191a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CharLSTM(\n",
            "  (LSTM): LSTM(27, 400, num_layers=4, batch_first=True, dropout=0.05)\n",
            "  (dropout): Dropout(p=0.05, inplace=False)\n",
            "  (fc): Linear(in_features=400, out_features=27, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "hidden_dim = 400\n",
        "dropout_prob=0.05\n",
        "num_layers=4\n",
        "vocab_size = len(char_to_int)\n",
        "model = CharLSTM(num_layers, vocab_size, hidden_dim, vocab_size, dropout_prob)\n",
        "model = model.to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "c738f5ec-9833-495d-b3e1-c0b61d31dee6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c738f5ec-9833-495d-b3e1-c0b61d31dee6",
        "outputId": "ea28c798-d809-47b4-ae0a-d8013eee7226"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/50: 100%|██████████| 53/53 [00:03<00:00, 14.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Loss: 2.8512\n",
            "Your trained model at epoch 0 is saved successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/50: 100%|██████████| 53/53 [00:03<00:00, 14.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/50, Loss: 2.8082\n",
            "Your trained model at epoch 1 is saved successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/50: 100%|██████████| 53/53 [00:03<00:00, 14.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/50, Loss: 2.8074\n",
            "Your trained model at epoch 2 is saved successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/50: 100%|██████████| 53/53 [00:03<00:00, 14.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/50, Loss: 2.7783\n",
            "Your trained model at epoch 3 is saved successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/50: 100%|██████████| 53/53 [00:03<00:00, 14.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/50, Loss: 2.4978\n",
            "Your trained model at epoch 4 is saved successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/50: 100%|██████████| 53/53 [00:03<00:00, 13.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/50, Loss: 2.2357\n",
            "Your trained model at epoch 5 is saved successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/50: 100%|██████████| 53/53 [00:03<00:00, 14.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/50, Loss: 2.0672\n",
            "Your trained model at epoch 6 is saved successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/50: 100%|██████████| 53/53 [00:03<00:00, 14.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/50, Loss: 1.9465\n",
            "Your trained model at epoch 7 is saved successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/50: 100%|██████████| 53/53 [00:03<00:00, 13.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/50, Loss: 1.8551\n",
            "Your trained model at epoch 8 is saved successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/50: 100%|██████████| 53/53 [00:03<00:00, 13.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/50, Loss: 1.7737\n",
            "Your trained model at epoch 9 is saved successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/50: 100%|██████████| 53/53 [00:03<00:00, 13.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/50, Loss: 1.6933\n",
            "Your trained model at epoch 10 is saved successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/50: 100%|██████████| 53/53 [00:03<00:00, 13.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/50, Loss: 1.6224\n",
            "Your trained model at epoch 11 is saved successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/50: 100%|██████████| 53/53 [00:03<00:00, 13.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/50, Loss: 1.5574\n",
            "Your trained model at epoch 12 is saved successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/50: 100%|██████████| 53/53 [00:03<00:00, 13.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/50, Loss: 1.5016\n",
            "Your trained model at epoch 13 is saved successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/50: 100%|██████████| 53/53 [00:03<00:00, 13.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/50, Loss: 1.4535\n",
            "Your trained model at epoch 14 is saved successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/50: 100%|██████████| 53/53 [00:03<00:00, 13.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/50, Loss: 1.4164\n",
            "Your trained model at epoch 15 is saved successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/50: 100%|██████████| 53/53 [00:03<00:00, 13.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/50, Loss: 1.3816\n",
            "Your trained model at epoch 16 is saved successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/50: 100%|██████████| 53/53 [00:03<00:00, 13.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/50, Loss: 1.3379\n",
            "Your trained model at epoch 17 is saved successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/50: 100%|██████████| 53/53 [00:03<00:00, 13.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/50, Loss: 1.3009\n",
            "Your trained model at epoch 18 is saved successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/50: 100%|██████████| 53/53 [00:03<00:00, 13.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/50, Loss: 1.2704\n",
            "Your trained model at epoch 19 is saved successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21/50: 100%|██████████| 53/53 [00:03<00:00, 13.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21/50, Loss: 1.2446\n",
            "Your trained model at epoch 20 is saved successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22/50: 100%|██████████| 53/53 [00:03<00:00, 13.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22/50, Loss: 1.2220\n",
            "Your trained model at epoch 21 is saved successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23/50: 100%|██████████| 53/53 [00:03<00:00, 13.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23/50, Loss: 1.1945\n",
            "Your trained model at epoch 22 is saved successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24/50: 100%|██████████| 53/53 [00:03<00:00, 13.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24/50, Loss: 1.1637\n",
            "Your trained model at epoch 23 is saved successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25/50: 100%|██████████| 53/53 [00:03<00:00, 13.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25/50, Loss: 1.1373\n",
            "Your trained model at epoch 24 is saved successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26/50: 100%|██████████| 53/53 [00:03<00:00, 13.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26/50, Loss: 1.1170\n",
            "Your trained model at epoch 25 is saved successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 27/50: 100%|██████████| 53/53 [00:03<00:00, 13.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27/50, Loss: 1.0977\n",
            "Your trained model at epoch 26 is saved successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 28/50: 100%|██████████| 53/53 [00:03<00:00, 13.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28/50, Loss: 1.0847\n",
            "Your trained model at epoch 27 is saved successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 29/50: 100%|██████████| 53/53 [00:03<00:00, 13.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29/50, Loss: 1.0625\n",
            "Your trained model at epoch 28 is saved successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 30/50: 100%|██████████| 53/53 [00:03<00:00, 13.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30/50, Loss: 1.0468\n",
            "Your trained model at epoch 29 is saved successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 31/50: 100%|██████████| 53/53 [00:03<00:00, 13.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 31/50, Loss: 1.0190\n",
            "Your trained model at epoch 30 is saved successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 32/50: 100%|██████████| 53/53 [00:03<00:00, 13.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 32/50, Loss: 0.9957\n",
            "Your trained model at epoch 31 is saved successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 33/50: 100%|██████████| 53/53 [00:03<00:00, 13.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 33/50, Loss: 0.9769\n",
            "Your trained model at epoch 32 is saved successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 34/50: 100%|██████████| 53/53 [00:03<00:00, 13.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 34/50, Loss: 0.9598\n",
            "Your trained model at epoch 33 is saved successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 35/50: 100%|██████████| 53/53 [00:03<00:00, 13.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 35/50, Loss: 0.9400\n",
            "Your trained model at epoch 34 is saved successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 36/50: 100%|██████████| 53/53 [00:03<00:00, 13.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 36/50, Loss: 0.9183\n",
            "Your trained model at epoch 35 is saved successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 37/50: 100%|██████████| 53/53 [00:03<00:00, 13.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 37/50, Loss: 0.8884\n",
            "Your trained model at epoch 36 is saved successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 38/50: 100%|██████████| 53/53 [00:03<00:00, 13.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 38/50, Loss: 0.8665\n",
            "Your trained model at epoch 37 is saved successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 39/50: 100%|██████████| 53/53 [00:03<00:00, 13.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 39/50, Loss: 0.8450\n",
            "Your trained model at epoch 38 is saved successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 40/50: 100%|██████████| 53/53 [00:03<00:00, 13.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 40/50, Loss: 0.8225\n",
            "Your trained model at epoch 39 is saved successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 41/50: 100%|██████████| 53/53 [00:03<00:00, 13.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 41/50, Loss: 0.7936\n",
            "Your trained model at epoch 40 is saved successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 42/50: 100%|██████████| 53/53 [00:03<00:00, 14.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 42/50, Loss: 0.7663\n",
            "Your trained model at epoch 41 is saved successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 43/50: 100%|██████████| 53/53 [00:03<00:00, 13.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 43/50, Loss: 0.7414\n",
            "Your trained model at epoch 42 is saved successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 44/50: 100%|██████████| 53/53 [00:03<00:00, 13.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 44/50, Loss: 0.7160\n",
            "Your trained model at epoch 43 is saved successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 45/50: 100%|██████████| 53/53 [00:03<00:00, 13.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 45/50, Loss: 0.6894\n",
            "Your trained model at epoch 44 is saved successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 46/50: 100%|██████████| 53/53 [00:03<00:00, 13.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 46/50, Loss: 0.6655\n",
            "Your trained model at epoch 45 is saved successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 47/50: 100%|██████████| 53/53 [00:03<00:00, 13.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 47/50, Loss: 0.6427\n",
            "Your trained model at epoch 46 is saved successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 48/50: 100%|██████████| 53/53 [00:03<00:00, 13.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 48/50, Loss: 0.6277\n",
            "Your trained model at epoch 47 is saved successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 49/50: 100%|██████████| 53/53 [00:03<00:00, 13.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 49/50, Loss: 0.6199\n",
            "Your trained model at epoch 48 is saved successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 50/50: 100%|██████████| 53/53 [00:03<00:00, 13.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50/50, Loss: 0.6072\n",
            "Your trained model at epoch 49 is saved successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 50\n",
        "batch_size = 50\n",
        "seq_length=100\n",
        "step_size=50\n",
        "learning_rate=0.001\n",
        "if not skip_training:\n",
        "    loss = train(\n",
        "        model=model,\n",
        "        encoded_chars=encoded_chars,\n",
        "        vocab_size=vocab_size,\n",
        "        num_epochs=num_epochs,\n",
        "        batch_size=batch_size,\n",
        "        seq_length=seq_length,\n",
        "        step_size=step_size,\n",
        "        learning_rate=learning_rate,\n",
        "        save_path='best_model.pth'\n",
        "    )\n",
        "else:\n",
        "    model.load_state_dict(torch.load('best_model.pth', weights_only=False, map_location=device))\n",
        "    print('Loaded weights from your saved model successfully!')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "338fcf4c-4b13-466f-bb2a-9fc79d276910",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "338fcf4c-4b13-466f-bb2a-9fc79d276910",
        "nbgrader": {
          "checksum": "5f4810bbcce1d9b751981d42d7cec2e3",
          "grade": false,
          "grade_id": "cell-2c5996453d5e58cb",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "### Step 6: Text Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "af09b58d-db73-438c-8146-2d3de3c7258c",
      "metadata": {
        "deletable": false,
        "id": "af09b58d-db73-438c-8146-2d3de3c7258c",
        "nbgrader": {
          "checksum": "f150ea2ef8cc1bf48f64175bcab64bd2",
          "grade": false,
          "grade_id": "cell-f6a72e59083bfbfb",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def generate_text(model, start_str, char_to_int, int_to_char, vocab_size, predict_len=100, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Generate text using the trained model.\n",
        "\n",
        "    Arguments:\n",
        "    model -- Trained RNN model\n",
        "    start_str -- String to start generating from\n",
        "    char_to_int -- Dictionary mapping characters to integers\n",
        "    int_to_char -- Dictionary mapping integers back to characters\n",
        "    vocab_size -- Size of the vocabulary\n",
        "    predict_len -- Number of characters to generate\n",
        "    temperature -- Float controlling randomness in predictions (higher is more random)\n",
        "\n",
        "    Returns:\n",
        "    generated_text -- The generated text as a string\n",
        "    \"\"\"\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    # Encode the starting string\n",
        "    input_seq = [char_to_int[char] for char in start_str]\n",
        "    input_seq = torch.tensor(input_seq).long().to(device).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "    hidden = model.init_hidden(1)  # Batch size of 1 for generating text\n",
        "\n",
        "    generated_text = start_str\n",
        "\n",
        "    with torch.no_grad():  # inference\n",
        "        for _ in range(predict_len):\n",
        "\n",
        "            # Make sure to have consistent variable naming with the rest of the code\n",
        "            x_one_hot = F.one_hot(input_seq, num_classes=vocab_size).float()\n",
        "            output, hidden = model(x_one_hot, hidden)\n",
        "            output = output[:, -1, :] / temperature\n",
        "\n",
        "            # Convert output to probabilities using softmax\n",
        "            probabilities = F.softmax(output, dim=-1).detach().cpu().numpy()\n",
        "\n",
        "            # Randomly sample based on the output probabilities\n",
        "            next_char_index = np.random.choice(range(vocab_size), p=probabilities.ravel())\n",
        "\n",
        "            # Add the predicted character to the generated text\n",
        "            next_char = int_to_char[next_char_index]\n",
        "            generated_text += next_char\n",
        "\n",
        "            # Update the input sequence - shift left to preserve the input length and add the new character\n",
        "            input_seq = torch.cat([input_seq[:, 1:], torch.tensor([[next_char_index]]).to(device)], dim=1)\n",
        "\n",
        "    return generated_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "cad2f2ef-fd10-484e-9824-aa90c90b1884",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cad2f2ef-fd10-484e-9824-aa90c90b1884",
        "outputId": "fa55488a-1166-4275-d590-88f13ff13d77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "alice was going to alice s very sore said the king heard it must be a lobster as the dormouse say the queen and the dormouse found herself would not could not so mad that s the mock turtle sound on the lizard said the hatter he was only tasted away some tarts and said to the table says and the queen and the first came to mine the gryphon repeat the queen said the king said the king heard it means to make out what i see said the king the queen said the king heard it be the time the queen said the mock turtle said alice was of very wide but she was not a bit said the king the book cauce of course the king heard it he turn the other suddenly that s the mock turtle said alice went to see what was the white rabbit mad be said the king went on the game of we said as she had got so she helt serpent she did not dare said alice was going to alice the mock turtle nive with the queen said the king shan t got in the window that s give it ever said the king said alice was just begins with his note book said \n"
          ]
        }
      ],
      "source": [
        "start_str = 'alice was '\n",
        "predict_len = 1000\n",
        "temperature = 0.5\n",
        "generated_text = generate_text(model,\n",
        "                               start_str,\n",
        "                               char_to_int,\n",
        "                               int_to_char,\n",
        "                               vocab_size,\n",
        "                               predict_len=predict_len,\n",
        "                               temperature=temperature)\n",
        "print(generated_text)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dataml200",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}